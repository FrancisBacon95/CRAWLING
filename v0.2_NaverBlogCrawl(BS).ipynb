{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "naver_Client_id = \"E4zDnlSmVAnJ7dvIOUUx\"\n",
    "naver_Client_secret = \"lLxEw7DWbB\"\n",
    "\n",
    "\n",
    "def get_blog_count(query,display) :\n",
    "    encode_query=urllib.parse.quote(query)\n",
    "    search_url=\"https://openapi.naver.com/v1/search/blog?query=\"+ encode_query +\"&display=\" + str(display)\n",
    "    # 위의 https://~ 는 따온 API호출예제에 있음.\n",
    "    request=urllib.request.Request(search_url)\n",
    "    # 이렇게 하면 내가 만들었던 url 형태로 요청한다.\n",
    "    # 그러나 이렇게 하면 API로부터 거부당한다.\n",
    "    # 왜냐하면 ID, Secret을 주지 않았기 때문에 아래와 같이 이것을 입력해준다.\n",
    "    \n",
    "    request.add_header(\"X-Naver-Client-Id\",naver_Client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",naver_Client_secret)\n",
    "    request.add_header(\"User-Agent\",'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36')    \n",
    "\n",
    "    response=urllib.request.urlopen(request)\n",
    "    response_code=response.getcode()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # code를 받아와서 웹에서 정상적으로 접속했을 때 200번을 준다.\n",
    "    if response_code is 200 :\n",
    "        response_body = response.read()  # 실제로 읽어 들인다.\n",
    "        response_body_dict = json.loads(response_body.decode('utf-8'))\n",
    "        # 네이버에서 넘겨줄 때 json으로 넘겨주는데 이것을 전부 '딕셔너리'로 저장한다.\n",
    "        \n",
    "        #print('Last build date :' + str(response_body_dict['lastBuildDate']))\n",
    "        # json으로 넘겨받을 것들 중에서 최종날짜를 출력\n",
    "        #print('Total :' + str(response_body_dict['total']))  # 관측개수\n",
    "        #print('Start :' + str(response_body_dict['start']))\n",
    "        #print('Display :' + str(response_body_dict['display']))\n",
    "        #이렇게 넘어온 것을 다 출력해본다.\n",
    "        \n",
    "        # 검색된 결과가 0 일 경우        \n",
    "        if response_body_dict['total']== 0:\n",
    "            blog_count = 0\n",
    "        else :\n",
    "            blog_total=math.ceil(response_body_dict['total']/int(display))\n",
    "            # 위에서 한 번에 10개씩 출력한다고 했으니 나눠주어서 개수를 맞춰준다.\n",
    "        \n",
    "            if blog_total >= 1000 :\n",
    "                blog_count = 1000\n",
    "                # 최대 1000개로 제한이 있으니 1000개를 넘었을 때는 1000개로 지정한다.\n",
    "            else : \n",
    "                blog_count = blog_total\n",
    "        \n",
    "            print('Blog total :' + str(blog_total))\n",
    "            print('Blog count : ' + str(blog_count))\n",
    "        \n",
    "        one_query_information_list.append(query)\n",
    "        one_query_information_list.append(str(response_body_dict['lastBuildDate']))\n",
    "        one_query_information_list.append(str(response_body_dict['total']))\n",
    "        one_query_information_list.append(str(blog_total))\n",
    "        one_query_information_list.append(str(blog_count))\n",
    "        return blog_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_post(query,display,start_index,sort):\n",
    "    global no, fs\n",
    "    #get_blog_count(query,display)를 정의할 때 사용했던 것과 동일하게 사용(아래 7문장)\n",
    "    encode_query = urllib.parse.quote(query)\n",
    "    # 동일하게 사용하나 search_url에는 옵션을 조금 더 추가해 준다.\n",
    "    search_url = \"https://openapi.naver.com/v1/search/blog?query=\" + encode_query +\"&display=\" + str(display) + \"&start=\" + str(start_index) + \"sort=\" + sort\n",
    "    request=urllib.request.Request(search_url)\n",
    "    \n",
    "    request.add_header(\"X-Naver-Client-Id\",naver_Client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",naver_Client_secret)\n",
    "    request.add_header(\"User-Agent\",'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36')\n",
    "    response=urllib.request.urlopen(request)\n",
    "    response_code=response.getcode()\n",
    "    \n",
    "    #print(response_code) 이분은 윗부분까지 해서 접속이 잘되었는지를 확인하는 것.\n",
    "        \n",
    "    if response_code is 200:\n",
    "        response_body=response.read()\n",
    "        response_body_dict=json.loads(response_body.decode('utf-8'))\n",
    "\n",
    "        for item_index in range(0,len(response_body_dict['items'])):\n",
    "\n",
    "            try :\n",
    "                remove_html_tag = re.compile('<.*?>')\n",
    "                title = re.sub(remove_html_tag,'',response_body_dict['items'][item_index]['title'])\n",
    "                #또한 주소가 따라오는데 이건 필요 없는 것이다.그러므로 없애준다.\n",
    "                link = response_body_dict['items'][item_index]['link'].replace(\"amp;\",\"\")\n",
    "                # description에도 마찬가지로 태그가 따라올 수 있기 때문에 제거해준다.\n",
    "                description = re.sub(remove_html_tag,'',response_body_dict['items'][item_index]['description'])\n",
    "                blogger_name = response_body_dict['items'][item_index]['bloggername']\n",
    "                #  포스트(개시글)의 주소가 아니라 블로그 자체의 링크를 가져옴.\n",
    "                blogger_link = response_body_dict['items'][item_index]['bloggerlink']\n",
    "                post_date = response_body_dict['items'][item_index]['postdate']\n",
    "                #아래처럼 변환해서 가져와도 되고 그냥 주는대로 가져와도 된다.\n",
    "                #post_date = datetime.datetime.strptime(response_body_dict[['item'][item_index]['postdate'],\"%Y%m%d\").strtime(\"%Y.%m.%d\")\n",
    "\n",
    "                no +=1\n",
    "    \n",
    "                #print(\"-------------------------------------------------\") #각각의 게시글을 구분시킴\n",
    "                #print(\"#\"+ str(no))  #몇번째 게시글인지 출력\n",
    "                #print(\"Title :\"+ title)\n",
    "                #print(\"Link :\"+ link)\n",
    "                #print(\"Description :\"+ description)\n",
    "                #print(\"Blogger Name :\"+ blogger_name)\n",
    "                #print(\"Blogger Link :\"+ blogger_link)\n",
    "                #print(\"Post Date :\"+ post_date)\n",
    "                \n",
    "                if int(post_date) >= 20170101 :\n",
    "                \n",
    "                    post_code = requests.get(link)\n",
    "                    post_text = post_code.text\n",
    "                    post_soup = BeautifulSoup(post_text,'lxml')\n",
    "    \n",
    "                    for mainFrame in post_soup.select('iframe#mainFrame'):\n",
    "                        blog_post_url = \"http://blog.naver.com\"+mainFrame.get('src')\n",
    "                        blog_post_code = requests.get(blog_post_url)\n",
    "                        blog_post_text = blog_post_code.text\n",
    "                        blog_post_soup = BeautifulSoup(blog_post_text,'lxml')\n",
    "                        \n",
    "                        for selector in ['div#postViewArea','.se-main-container','div#contentDiv','div#se_component_wrap sect_dsc __se_component_area']:\n",
    "                            for pc in blog_post_soup.select(selector):\n",
    "                                if len(str(pc.get_text())) > 2:\n",
    "                                    pc_text = pc.get_text()\n",
    "                                    pc_text2 = str(pc_text).replace(\"\\n\",'')  #컨텐트를 string으로 캐스팅해서 가져온다.\n",
    "                                    pc_text2 = str(pc_text).replace(\"\\u200b\",'')\n",
    "                                    #print(pc_text2)\n",
    "                                \n",
    "                                else:\n",
    "                                    continue\n",
    "                                #print([title,post_date,blogger_name,description,link,pc_text2])\n",
    "                                csv_writer.writerow([title,post_date,blogger_name,description,link,pc_text2])\n",
    "                                \n",
    "                         \n",
    "            #만약에 못할 경우 그냥 넘어가는 코드.\n",
    "            except: \n",
    "                item_index+=1\n",
    "                print(\"■■■■■■저장할 수 없음.■■■■■■\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_pd=pd.read_csv(\"네이버Blog 키워드.csv\",encoding='cp949')\n",
    "#query_list=query_pd['품명'].values.tolist()\n",
    "query_list=['창원']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "창원시작\n",
      "Blog total :361663\n",
      "Blog count : 1000\n",
      "  query                  Last build date    Total Blog total Blog count  \\\n",
      "0    창원  Thu, 29 Aug 2019 15:29:42 +0900  3616626     361663       1000   \n",
      "\n",
      "  stored data  \n",
      "0         962  \n"
     ]
    }
   ],
   "source": [
    "   \n",
    "\n",
    "no = 0           # 몇 개의 포스트를 저장했는지 세기 위한 index\n",
    "#query = \"대구\" # 검색을 원하는 문자열로써 UTF-8로 인코딩한다.\n",
    "display = 10    # 검색 결과 출력 건수 지정, 10(기본값), 100(최대값)\n",
    "start = 1        # 검색 시작 위치로 최대 1000까지 가능\n",
    "sort = 'date'    # 정렬 옵션 : sim(유사도순, 기본값), date(날짜순)\n",
    "\n",
    "all_query_information_pd=pd.DataFrame(columns=['query','Last build date','Total','Blog total','Blog count','stored data'])     \n",
    "for query in query_list :\n",
    "    print(query+\"시작\")\n",
    "    fs = open(\"./NaverBlogData_v0.2/v0.2_\"+query + \".csv\",'w',encoding='utf-8',newline=\"\")\n",
    "    csv_writer=csv.writer(fs)\n",
    "    csv_writer.writerow([\"title\",\"post_date\",\"blogger_name\",\"description\",\"link\",\"content\"])\n",
    "    one_query_information_list=[]\n",
    "    \n",
    "    blog_count = get_blog_count(query,display)\n",
    "    \n",
    "    for start_index in range(start,blog_count + 1, display):\n",
    "        get_blog_post(query,display,start_index,sort)  \n",
    "    \n",
    "    fs.close()\n",
    "    data=pd.read_csv(\"./NaverBlogData_v0.2/v0.2_\"+query+\".csv\",encoding='utf-8')\n",
    "    one_query_information_list.append(len(data))\n",
    "    all_query_information_pd.loc[len(all_query_information_pd)]=one_query_information_list\n",
    "    print(all_query_information_pd)\n",
    "    \n",
    "all_query_information_pd.to_csv(\"./NaverBlogData_v0.2/\"+\"v0.2_all_query_information.csv\",encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
