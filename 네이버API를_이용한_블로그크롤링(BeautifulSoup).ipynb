{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 블로그 크롤링 (네이버 API 이용)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 들어가며...\n",
    "\n",
    "\n",
    "\n",
    "## 1) 크롤링? 파싱?\n",
    "\n",
    "### 1-1) 크롤링\n",
    "\n",
    "- '웹 크롤러'라는 단어에서 유래되었음.\n",
    "- 크롤러란 조직적, 자동화된 방법으로 WWW를 탐색하는 컴퓨터 프로그램.\n",
    "- 크롤링은 크롤러가 하는 작업을 부르는 말.\n",
    "- 여러 인터넷 사이트의 페이지(문서,html 등)를 수집해서 분류하는 것.\n",
    "- 대체로 찾아낸 데이터를 저장한 후 쉽게 찾을 수 있게 인덱싱 수행.\n",
    "\n",
    "### 1-2) 파싱\n",
    "\n",
    "- 파싱이란 어떤 페이지(문서, html 등)에서 내가 원하는 데이터를 특정 패턴이나 순서로 추출하여 정보를 가공하는 것.\n",
    "- 파싱이란 일련의 문자열을 의미있는 '토큰'으로 분해하고 이들로 이루어진 '파스 트리'를 만드는 과정.\n",
    "- 입력 토큰에 내제된 자료 구조를 빌드하고 문법을 검사하는 역할을 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 애플리케이션 등록 ( API 이용신청)\n",
    "\n",
    "![애플리케이션 등록_1](https://user-images.githubusercontent.com/51112316/61574953-2f655d80-ab01-11e9-8b8f-cc74183302da.jpg)\n",
    "![애플리케이션 등록_2](https://user-images.githubusercontent.com/51112316/61574954-31c7b780-ab01-11e9-9274-e1d237547d94.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1) 네이버API  예제 사용\n",
    "\n",
    "- 네이버개발자 -> Products -> 서비스API -> 검색 -> 개발 가이드 보기 -> 0.API호출예제 -> Python\n",
    "\n",
    "#### [API호출예제](https://developers.naver.com/docs/search/blog/)\n",
    "\n",
    "~~~python\n",
    "import java.io.BufferedReader;\n",
    "import java.io.InputStreamReader;\n",
    "import java.net.HttpURLConnection;\n",
    "import java.net.URL;\n",
    "import java.net.URLEncoder;\n",
    "\n",
    "public class APIExamSearchBlog {\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        String clientId = \"YOUR_CLIENT_ID\";//애플리케이션 클라이언트 아이디값\";\n",
    "        String clientSecret = \"YOUR_CLIENT_SECRET\";//애플리케이션 클라이언트 시크릿값\";\n",
    "        try {\n",
    "            String text = URLEncoder.encode(\"그린팩토리\", \"UTF-8\");\n",
    "            String apiURL = \"https://openapi.naver.com/v1/search/blog?query=\"+ text; // json 결과\n",
    "            //String apiURL = \"https://openapi.naver.com/v1/search/blog.xml?query=\"+ text; // xml 결과\n",
    "            URL url = new URL(apiURL);\n",
    "            HttpURLConnection con = (HttpURLConnection)url.openConnection();\n",
    "            con.setRequestMethod(\"GET\");\n",
    "            con.setRequestProperty(\"X-Naver-Client-Id\", clientId);\n",
    "            con.setRequestProperty(\"X-Naver-Client-Secret\", clientSecret);\n",
    "            int responseCode = con.getResponseCode();\n",
    "            BufferedReader br;\n",
    "            if(responseCode==200) { // 정상 호출\n",
    "                br = new BufferedReader(new InputStreamReader(con.getInputStream()));\n",
    "            } else {  // 에러 발생\n",
    "                br = new BufferedReader(new InputStreamReader(con.getErrorStream()));\n",
    "            }\n",
    "            String inputLine;\n",
    "            StringBuffer response = new StringBuffer();\n",
    "            while ((inputLine = br.readLine()) != null) {\n",
    "                response.append(inputLine);\n",
    "            }\n",
    "            br.close();\n",
    "            System.out.println(response.toString());\n",
    "        } catch (Exception e) {\n",
    "            System.out.println(e);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제 발생 :  \n",
    "\n",
    "#### 네이버 블로그 형식 맞춰 긁어왔음.\n",
    "                  \n",
    "#### 사실은 요약된 내용.\n",
    "\n",
    "#### 모든 내용을 다 보고싶으면 개발이 필요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 블로그 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "naver_Client_id = \"E4zDnlSmVAnJ7dvIOUUx\"\n",
    "naver_Client_secret = \"lLxEw7DWbB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_count(query,display) :\n",
    "    encode_query=urllib.parse.quote(query)\n",
    "    search_url=\"https://openapi.naver.com/v1/search/blog?query=\"+ encode_query\n",
    "    # 위의 https://~ 는 따온 API호출예제에 있음.\n",
    "    request=urllib.request.Request(search_url)\n",
    "    # 이렇게 하면 내가 만들었던 url 형태로 요청한다.\n",
    "    # 그러나 이렇게 하면 API로부터 거부당한다.\n",
    "    # 왜냐하면 ID, Secret을 주지 않았기 때문에 아래와 같이 이것을 입력해준다.\n",
    "    \n",
    "    request.add_header(\"X-Naver-Client-Id\",naver_Client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",naver_Client_secret)\n",
    "    \n",
    "    response=urllib.request.urlopen(request)\n",
    "    response_code=response.getcode()\n",
    "    \n",
    "    # code를 받아와서 웹에서 정상적으로 접속했을 때 200번을 준다.\n",
    "    if response_code is 200 :\n",
    "        response_body = response.read()  # 실제로 읽어 들인다.\n",
    "        response_body_dict = json.loads(response_body.decode('utf-8'))\n",
    "        # 네이버에서 넘겨줄 때 json으로 넘겨주는데 이것을 전부 '딕셔너리'로 저장한다.\n",
    "        \n",
    "        print('Last build date :' + str(response_body_dict['lastBuildDate']))\n",
    "        # json으로 넘겨받을 것들 중에서 최종날짜를 출력\n",
    "        print('Total :' + str(response_body_dict['total']))  # 관측개수\n",
    "        print('Start :' + str(response_body_dict['start']))\n",
    "        print('Display :' + str(response_body_dict['display']))\n",
    "        #이렇게 넘어온 것을 다 출력해본다.\n",
    "        \n",
    "        # 검색된 결과가 0 일 경우        \n",
    "        if response_body_dict['total']== 0:\n",
    "            blog_count = 0\n",
    "        else :\n",
    "            blog_total=math.ceil(response_body_dict['total']/int(display))\n",
    "            # 위에서 한 번에 10개씩 출력한다고 했으니 나눠주어서 개수를 맞춰준다.\n",
    "        \n",
    "            if blog_total >= 1000 :\n",
    "                blog_count = 1000\n",
    "                # 최대 1000개로 제한이 있으니 1000개를 넘었을 때는 1000개로 지정한다.\n",
    "            else : \n",
    "                blog_count = blog_total\n",
    "        \n",
    "            print('Blog total :' + str(blog_total))\n",
    "            print('Blog count : ' + str(blog_count))\n",
    "        \n",
    "        return blog_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 블로그의 모든 내용을 가져오는 함수[(API사용예제)는 샘플 내용만 가져온다.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_blog_post(query,display,start_index,sort):\n",
    "    global no, fs\n",
    "    #get_blog_count(query,display)를 정의할 때 사용했던 것과 동일하게 사용(아래 7문장)\n",
    "    encode_query = urllib.parse.quote(query)\n",
    "    # 동일하게 사용하나 search_url에는 옵션을 조금 더 추가해 준다.\n",
    "    search_url = \"https://openapi.naver.com/v1/search/blog?query=\" + encode_query +\"&display=\" + str(display) + \"&start=\" + str(start_index) + \"sort=\" + sort\n",
    "    request=urllib.request.Request(search_url)\n",
    "    \n",
    "    request.add_header(\"X-Naver-Client-Id\",naver_Client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",naver_Client_secret)\n",
    "    \n",
    "    response=urllib.request.urlopen(request)\n",
    "    response_code=response.getcode()\n",
    "    \n",
    "    #print(response_code) 이분은 윗부분까지 해서 접속이 잘되었는지를 확인하는 것.\n",
    "        \n",
    "    if response_code is 200:\n",
    "        response_body=response.read()\n",
    "        response_body_dict=json.loads(response_body.decode('utf-8'))\n",
    "        # json이 해당하는 블로그가 여러개 나오면 전부 다 하나의 큰 덩어리로 주는데\n",
    "        # 나눠서 하나씩 접근 하기 위해 아래의 코드를 짜줌.\n",
    "        for item_index in range(0,len(response_body_dict['items'])):\n",
    "            # 혹시 태그부분이 따라올 수 있기 때문에\n",
    "            # 그것을 지우고 사용하기 위해  re(regular expression)으로 제거를 해준다.\n",
    "            # 즉 태그를 없애주는 정규표현식을 이용한다.\n",
    "            try :\n",
    "                remove_html_tag = re.compile('<.*?>')\n",
    "                title = re.sub(remove_html_tag,'',response_body_dict['items'][item_index]['title'])\n",
    "                #또한 주소가 따라오는데 이건 필요 없는 것이다.그러므로 없애준다.\n",
    "                link = response_body_dict['items'][item_index]['link'].replace(\"amp;\",\"\")\n",
    "                # description에도 마찬가지로 태그가 따라올 수 있기 때문에 제거해준다.\n",
    "                description = re.sub(remove_html_tag,'',response_body_dict['items'][item_index]['description'])\n",
    "                blogger_name = response_body_dict['items'][item_index]['bloggername']\n",
    "                #  포스트(개시글)의 주소가 아니라 블로그 자체의 링크를 가져옴.\n",
    "                blogger_link = response_body_dict['items'][item_index]['bloggerlink']\n",
    "                post_date = response_body_dict['items'][item_index]['postdate']\n",
    "                #아래처럼 변환해서 가져와도 되고 그냥 주는대로 가져와도 된다.\n",
    "                #post_date = datetime.datetime.strptime(response_body_dict[['item'][item_index]['postdate'],\"%Y%m%d\").strtime(\"%Y.%m.%d\")\n",
    "                \n",
    "               # 이제 json에서 오는 결과를 하나씩 items에 다 들어가 있다. 그것을 이제 다 끄집어서 가져온다.\n",
    "                no +=1\n",
    "                '''\n",
    "                print(\"-------------------------------------------------\") #각각의 게시글을 구분시킴\n",
    "                print(\"#\"+ str(no))  #몇번째 게시글인지 출력\n",
    "                print(\"Title :\"+ title)\n",
    "                print(\"Link :\"+ link)\n",
    "                print(\"Description :\"+ description)\n",
    "                print(\"Blogger Name :\"+ blogger_name)\n",
    "                print(\"Blogger Link :\"+ blogger_link)\n",
    "                print(\"Post Date :\"+ post_date)\n",
    "                '''\n",
    "                                                                              \n",
    "                post_code = requests.get(link)\n",
    "                #  저 정보들은 사실 네이버 오픈 API에서 제공해주는 정보다.\n",
    "                # 그런데 우리가 필요한 것은 실제 블로그에 포스트 된 내용이 필요하기 때문에\n",
    "                # 뷰티풀숲을 이용해 가져온다. \n",
    "                post_text = post_code.text\n",
    "                post_soup = BeautifulSoup(post_text,'lxml')\n",
    "                \n",
    "                # 네이버 블로그의 문제점 : 마우스를 긁어오지 못하게 iframe으로 만들어 놓은게 있다.\n",
    "                #그래서 iframe#mainFrame 이라는 html 부분을 들어가서\n",
    "                # 그 부분의 실제 post url을 뽑을 수 있다.\n",
    "                # 그냥 가져와선 안되고 그 url에서 소스부분(src)만 가져온다.\n",
    "                # 그래서 그걸로 접속해서 실제적인 블로그 포스트에 대한 내용을 긁어오기 때문에\n",
    "                # 여기에 다시 한번 \"blog_post_url\"이라는 주소로 접근을 한다.\n",
    "                # 그리고 그것들에 대한 정보를 가져온다.\n",
    "                for mainFrame in post_soup.select('iframe#mainFrame'):\n",
    "                    blog_post_url = \"http://blog.naver.com\"+mainFrame.get('src')\n",
    "                    blog_post_code = requests.get(blog_post_url)\n",
    "                    blog_post_text = blog_post_code.text\n",
    "                    blog_post_soup = BeautifulSoup(blog_post_text,'lxml')\n",
    "                    \n",
    "                    # 블로그 전체의 여러가지 메뉴들도 있고 그런 것 제외하고 postViewArea라는 부분에\n",
    "                    # 실제 포스트 내용이 들어있다. 그래서 여기로 선택한다. 이게 진짜 블로그 포스트의 컨텐트다. \n",
    "                    for blog_post_content in blog_post_soup.select(\"div#postViewArea\"):\n",
    "                        blog_post_content_text = blog_post_content.get_text()\n",
    "                        blog_post_full_contents = str(blog_post_content_text) #컨텐트를 string으로 캐스팅해서 가져온다.\n",
    "                        blog_post_full_contents = blog_post_full_contents.replace(\"\\n\\n\",'\\n') \n",
    "                        #요약된 description 만 주는게 아니라 이젠 진짜로 된 포스트를 준다.\n",
    "                        #print(\"blog_post_contents : \" + blog_post_full_contents+\"\\n\")\n",
    "                        #전체 내용을 print 하면 너무 많기 때문에 위의 코드로 하지않고 파일로 저장한다.\n",
    "                        #이를 통해 이후에 데이터 분석 등에 사용한다.\n",
    "                        fs.write(blog_post_full_contents+'\\n')\n",
    "                        fs.write(\"-----------------------------------\")\n",
    "            #만약에 못할 경우 그냥 넘어가는 코드.\n",
    "            except: \n",
    "                item_index+=1\n",
    "                print(\"■■■■■■저장할 수 없음.■■■■■■\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 아래 모든 코드를 자동화시키기 위해 query=\"키워드\"로 한 후에 shift+ENTER만 갈긴다.\n",
    "\n",
    "### - 위의 함수를 정의할 때 description을 프린트 하는 것을 주석 처리했기 때문에 코드 사용시에는 반드시 제거후 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"네이버 크롤링\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last build date :Sat, 27 Jul 2019 21:33:10 +0900\n",
      "Total :2855\n",
      "Start :1\n",
      "Display :10\n",
      "Blog total :286\n",
      "Blog count : 286\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__' :\n",
    "    no = 0           # 몇 개의 포스트를 저장했는지 세기 위한 index\n",
    "    #query = \"대구\" # 검색을 원하는 문자열로써 UTF-8로 인코딩한다.\n",
    "    display = 10     # 검색 결과 출력 건수 지정, 10(기본값), 100(최대값)\n",
    "    start = 1        # 검색 시작 위치로 최대 1000까지 가능\n",
    "    sort = 'date'    # 정렬 옵션 : sim(유사도순, 기본값), date(날짜순)\n",
    "    \n",
    "    #블로그 콘텐츠의 한글 저장을 위해 encoding='utf-8'으로 설정.\n",
    "    fs = open(query + \".txt\",'a',encoding='utf-8')\n",
    "    \n",
    "    blog_count = get_blog_count(query,display)\n",
    "    for start_index in range(start,blog_count + 1, display):\n",
    "        get_blog_post(query,display,start_index,sort)  \n",
    "    \n",
    "       \n",
    "    fs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mecab : window에서 안되는 걸로 알고 있음.\n",
    "- 한나눔 : 성능이 떨어지는 것으로 알고 있음\n",
    "- 카이(카카오 형태소 분석기) : 사전 추가가 안됨.\n",
    "- 트위터 분석기, 코모란 두 가지를 중점."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "from ckonlpy.tag import Postprocessor\n",
    "\n",
    "twitter = Twitter()\n",
    "okt_nouns = Postprocessor(twitter, passtags={'Noun'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='박대'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 입수\n",
    "text = open(query+'.txt',encoding='utf-8').read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt_result=okt_nouns.pos(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가적인 불용어(stopword) 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=pd.read_csv(\"한글불용어.csv\",encoding='utf-8')\n",
    "stopword=stopwords[\"불용어\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 명사 카운팅 Top100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=pd.DataFrame(okt_result)\n",
    "nostop_data=raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in stopword :\n",
    "    nostop_data=nostop_data[nostop_data[0]!=word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>박대</td>\n",
       "      <td>8357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>생선</td>\n",
       "      <td>3050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>맛</td>\n",
       "      <td>2177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>어요</td>\n",
       "      <td>1929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>군산</td>\n",
       "      <td>1724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>구이</td>\n",
       "      <td>1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>요</td>\n",
       "      <td>1247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>조림</td>\n",
       "      <td>1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>어서</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>마리</td>\n",
       "      <td>907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1\n",
       "0  박대  8357\n",
       "1  생선  3050\n",
       "2   맛  2177\n",
       "3  어요  1929\n",
       "4  군산  1724\n",
       "5  구이  1570\n",
       "6   요  1247\n",
       "7  조림  1197\n",
       "8  어서   979\n",
       "9  마리   907"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nostop_data_list=nostop_data[0].tolist()\n",
    "count=Counter(nostop_data_list)\n",
    "counting_data=pd.DataFrame(count.most_common(100))\n",
    "counting_data.to_csv(query+\"_카운팅.csv\",encoding=\"ms949\")\n",
    "counting_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 워드 클라우드 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec 적용\n",
    "\n",
    "- 내가 원하는 키워드로 크롤링한 모든 데이터를 학습시켜 만든 모델."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 크롤링된 데이터를 포스트 단위로 리스트 생성\n",
    "\n",
    "- 이후의 오류 방지를 위해 비어있는 포스트의 경우에는 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1070\n",
      "1069\n",
      "1069\n"
     ]
    }
   ],
   "source": [
    "post_list=text.split(\"-----------------------------------\")\n",
    "print(len(post_list))\n",
    "for i in range(len(post_list)) :\n",
    "    if(len(post_list[i]) == 0):\n",
    "        print(i)\n",
    "        del post_list[i]\n",
    "        \n",
    "print(len(post_list))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 각각의 포스트에 대하여 형태소 분석기를 돌려서 명사만을 저장 [ ( '단어' , '품사' ) , ...... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_list1=[]\n",
    "\n",
    "for post1 in post_list :\n",
    "    okt_post=okt_nouns.pos(post1)\n",
    "    post_list1.append(pd.DataFrame(okt_post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 저장된 명사 리스트[ ( '단어' , '품사' ) , ...... ]를 데이터프레임화 시킨 후 품사부분을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_list2=[]\n",
    "for item1 in post_list1 :\n",
    "    item2=pd.DataFrame(item1)\n",
    "    post_list2.append(item2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "post_list3=[]\n",
    "for df in post_list2 :\n",
    "    df2=pd.DataFrame(df[0])\n",
    "    post_list3.append(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) 불용어 제거 ( 산출물 :DataFrame -> List )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_list4=[]\n",
    "for df in post_list3 :\n",
    "    nonstop_df=df\n",
    "    for word in stopword :\n",
    "        nonstop_df=nonstop_df[nonstop_df[0]!=word]\n",
    "    post_list4.append(nonstop_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_list5=[]\n",
    "for df in post_list4 :\n",
    "    df_to_list=df[0].values.tolist()\n",
    "    post_list5.append(df_to_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Word2Vec 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(post_list5, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>종종</td>\n",
       "      <td>0.936352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>요리</td>\n",
       "      <td>0.931227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>미네</td>\n",
       "      <td>0.921317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>두둥</td>\n",
       "      <td>0.919748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>대조</td>\n",
       "      <td>0.912538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>리해</td>\n",
       "      <td>0.910852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>구이</td>\n",
       "      <td>0.907459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>요</td>\n",
       "      <td>0.902463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>러시</td>\n",
       "      <td>0.886372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>영감</td>\n",
       "      <td>0.886337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1\n",
       "0  종종  0.936352\n",
       "1  요리  0.931227\n",
       "2  미네  0.921317\n",
       "3  두둥  0.919748\n",
       "4  대조  0.912538\n",
       "5  리해  0.910852\n",
       "6  구이  0.907459\n",
       "7   요  0.902463\n",
       "8  러시  0.886372\n",
       "9  영감  0.886337"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends=model.wv.most_similar(query)\n",
    "finish=pd.DataFrame(friends)\n",
    "finish.to_csv(query+\"word2vec.csv\")\n",
    "finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
